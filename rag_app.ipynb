{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries"
      ],
      "metadata": {
        "id": "5MrCKDwPCaY2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJA_FAO4-7VR"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import urllib.request\n",
        "from litellm import completion\n",
        "import fitz\n",
        "import numpy as np\n",
        "import tensorflow_hub as hub\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from config import openAI_key\n",
        "import gradio as gr\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PDF text extraction and chunking"
      ],
      "metadata": {
        "id": "i6TlorLaCl2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "recommender = None\n",
        "\n",
        "def download_pdf(url, output_path):\n",
        "    urllib.request.urlretrieve(url, output_path)\n",
        "\n",
        "\n",
        "def preprocess(text):\n",
        "    \"\"\"\n",
        "    Preprocesses the input text by replacing newline characters with spaces\n",
        "    and reducing multiple spaces to a single space.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to preprocess.\n",
        "\n",
        "    Returns:\n",
        "        str: The preprocessed text with newline characters replaced by spaces\n",
        "             and multiple spaces reduced to a single space.\n",
        "    \"\"\"\n",
        "    text = text.replace('\\n', ' ')\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    return text\n",
        "\n",
        "def pdf_to_text(path, start_page=1, end_page=None):\n",
        "    \"\"\"\n",
        "    Extracts and preprocesses text from a PDF file within a specified page range.\n",
        "\n",
        "    Args:\n",
        "        path (str): The file path to the PDF document.\n",
        "        start_page (int, optional): The starting page number for text extraction (1-based index). Defaults to 1.\n",
        "        end_page (int, optional): The ending page number for text extraction (1-based index). If None, extracts until the last page. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of preprocessed text strings, each representing the text content of a page within the specified range.\n",
        "    \"\"\"\n",
        "    doc = fitz.open(path)\n",
        "    total_pages = doc.page_count\n",
        "\n",
        "    if end_page is None:\n",
        "        end_page = total_pages\n",
        "\n",
        "    text_list = []\n",
        "\n",
        "    for i in range(start_page - 1, end_page):\n",
        "        text = doc.load_page(i).get_text(\"text\")\n",
        "        text = preprocess(text)\n",
        "        text_list.append(text)\n",
        "\n",
        "    doc.close()\n",
        "    return text_list\n",
        "\n",
        "def text_to_chunks(texts, word_length=150, start_page=1):\n",
        "    \"\"\"\n",
        "    Splits the input texts into chunks of a specified word length and annotates each chunk with its page number.\n",
        "\n",
        "    Args:\n",
        "        texts (list of str): A list of text strings, each representing the content of a page.\n",
        "        word_length (int, optional): The number of words per chunk. Defaults to 150.\n",
        "        start_page (int, optional): The starting page number for annotation. Defaults to 1.\n",
        "\n",
        "    Returns:\n",
        "        list of str: A list of text chunks, each annotated with its corresponding page number.\n",
        "    \"\"\"\n",
        "    text_toks = [t.split(' ') for t in texts]\n",
        "    chunks = []\n",
        "\n",
        "    for idx, words in enumerate(text_toks):\n",
        "        for i in range(0, len(words), word_length):\n",
        "            chunk = words[i : i + word_length]\n",
        "            if (\n",
        "                (i + word_length) > len(words)\n",
        "                and (len(chunk) < word_length)\n",
        "                and (len(text_toks) != (idx + 1))\n",
        "            ):\n",
        "                text_toks[idx + 1] = chunk + text_toks[idx + 1]\n",
        "                continue\n",
        "            chunk = ' '.join(chunk).strip()\n",
        "            chunk = f'[Page no. {idx+start_page}]' + ' ' + '\"' + chunk + '\"'\n",
        "            chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QtsdEQVjCfk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create vector embeddings and fitting semantic search model"
      ],
      "metadata": {
        "id": "yV16w9upC0W9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SemanticSearch:\n",
        "    def __init__(self):\n",
        "        self.use = hub.load(r'C:\\Users\\u393845\\wns\\GenAI\\universal-sentence-encoder_4')\n",
        "        self.fitted = False\n",
        "\n",
        "    def fit(self, data, batch=1000, n_neighbors=5):\n",
        "        \"\"\"\n",
        "        Fits the semantic search model on the provided data by computing embeddings and training a nearest neighbors model.\n",
        "\n",
        "        Args:\n",
        "            data (list of str): The input data to fit the model on.\n",
        "            batch (int, optional): The batch size for processing the data into embeddings. Defaults to 1000.\n",
        "            n_neighbors (int, optional): The number of neighbors to use for the nearest neighbors model. Defaults to 5.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.embeddings = self.get_text_embedding(data, batch=batch)\n",
        "        n_neighbors = min(n_neighbors, len(self.embeddings))\n",
        "        self.nn = NearestNeighbors(n_neighbors=n_neighbors)\n",
        "        self.nn.fit(self.embeddings)\n",
        "        self.fitted = True\n",
        "\n",
        "    def __call__(self, text, return_data=True):\n",
        "        inp_emb = self.use([text])\n",
        "        neighbors = self.nn.kneighbors(inp_emb, return_distance=False)[0]\n",
        "\n",
        "        if return_data:\n",
        "            return [self.data[i] for i in neighbors]\n",
        "        else:\n",
        "            return neighbors\n",
        "\n",
        "    def get_text_embedding(self, texts, batch=1000):\n",
        "        \"\"\"\n",
        "        Computes embeddings for a list of text strings using a specified batch size.\n",
        "\n",
        "        Args:\n",
        "            texts (list of str): The input text strings to compute embeddings for.\n",
        "            batch (int, optional): The batch size for processing the texts. Defaults to 1000.\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: A 2D array where each row represents the embedding of a text string.\n",
        "        \"\"\"\n",
        "        embeddings = []\n",
        "        for i in range(0, len(texts), batch):\n",
        "            text_batch = texts[i : (i + batch)]\n",
        "            emb_batch = self.use(text_batch)\n",
        "            embeddings.append(emb_batch)\n",
        "        embeddings = np.vstack(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3YdQsLVVCfnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit recommender model with extracted text"
      ],
      "metadata": {
        "id": "F3DyE_3RDLUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_recommender(path, start_page=1):\n",
        "    \"\"\"\n",
        "    Loads and fits the recommender model with text data extracted from a PDF file.\n",
        "\n",
        "    Args:\n",
        "        path (str): The file path to the PDF document.\n",
        "        start_page (int, optional): The starting page number for text extraction (1-based index). Defaults to 1.\n",
        "\n",
        "    Returns:\n",
        "        str: A message indicating that the corpus has been loaded.\n",
        "    \"\"\"\n",
        "    global recommender\n",
        "    if recommender is None:\n",
        "        recommender = SemanticSearch()\n",
        "\n",
        "    texts = pdf_to_text(path, start_page=start_page)\n",
        "    chunks = text_to_chunks(texts, start_page=start_page)\n",
        "    recommender.fit(chunks)\n",
        "    return 'Corpus Loaded.'\n",
        "\n"
      ],
      "metadata": {
        "id": "C3HwzFWqCfp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating answer with augmented prompt"
      ],
      "metadata": {
        "id": "wZun25JyDbpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(prompt, engine=\"gpt-4o-mini\"):\n",
        "    try:\n",
        "        messages=[{ \"content\": prompt,\"role\": \"user\"}]\n",
        "        completions = completion(\n",
        "            model=engine,\n",
        "            messages=messages,\n",
        "            max_tokens=512,\n",
        "            n=1,\n",
        "            stop=None,\n",
        "            temperature=0.7,\n",
        "            api_key=openAI_key\n",
        "        )\n",
        "        message = completions['choices'][0]['message']['content']\n",
        "    except Exception as e:\n",
        "        message = f'API Error: {str(e)}'\n",
        "    return message\n",
        "\n",
        "\n",
        "def generate_answer(question):\n",
        "    \"\"\"\n",
        "    Generates an answer to a given question by querying the recommender model and formatting the results into a prompt.\n",
        "\n",
        "    Args:\n",
        "        question (str): The question to generate an answer for.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated answer based on the search results from the recommender model.\n",
        "    \"\"\"\n",
        "    topn_chunks = recommender(question)\n",
        "    prompt = \"\"\n",
        "    prompt += 'search results:\\n\\n'\n",
        "    for c in topn_chunks:\n",
        "        prompt += c + '\\n\\n'\n",
        "\n",
        "    prompt += (\n",
        "        \"Instructions: Compose a comprehensive reply to the query using the search results given. \"\n",
        "        \"Cite each reference using [Page Number] notation (every result has this number at the beginning). \"\n",
        "        \"Citation should be done at the end of each sentence. If the search results mention multiple subjects \"\n",
        "        \"with the same name, create separate answers for each. Only include information found in the results and \"\n",
        "        \"don't add any additional information. Make sure the answer is correct and don't output false content. \"\n",
        "        \"If the text does not relate to the query, simply state 'Text Not Found in PDF'. Ignore outlier \"\n",
        "        \"search results which has nothing to do with the question. Only answer what is asked. The \"\n",
        "        \"answer should be short and concise. Answer step-by-step.\"\n",
        "    )\n",
        "\n",
        "    prompt += f\"Query: {question}\\nAnswer:\"\n",
        "    answer = generate_text(prompt, \"gpt-4o-mini\")\n",
        "    return answer\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l6XQ3qnkCfsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Give input file name and list of questions and get the output in json blob"
      ],
      "metadata": {
        "id": "lRMx_fRwDu13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_file_name = 'handbook.pdf'\n",
        "questions = ['What is the name of the organization?',]\n",
        "             # 'Who is the CEO of the company?',\n",
        "             # 'What is their vacation policy?',\n",
        "             # 'What is the termination policy?',]\n",
        "\n",
        "load_recommender(input_file_name)\n",
        "\n",
        "results = {}\n",
        "for question in questions:\n",
        "    results[question] = generate_answer(question)\n",
        "\n",
        "# Convert the results dictionary to a JSON string\n",
        "json_output = json.dumps(results, indent=4)\n",
        "print(json_output)\n"
      ],
      "metadata": {
        "id": "QwUR0iZrCfv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Gradio demo"
      ],
      "metadata": {
        "id": "miWOYxF_Dskq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_question(uploaded_file, question):\n",
        "    load_recommender(uploaded_file.name)\n",
        "    answer = generate_answer(question, openAI_key)\n",
        "    return answer\n",
        "\n",
        "# question = \"What is the name of the company?\"\n",
        "# print(generate_answer(question, openAI_key))\n",
        "\n",
        "# Create the Gradio interface for demo purpose\n",
        "def gradio_interface(uploaded_file, question):\n",
        "    return handle_question(uploaded_file, question)\n",
        "\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=gradio_interface,\n",
        "    inputs=[\n",
        "        gr.File(label=\"Upload PDF\", file_types=[\".pdf\"]),\n",
        "        gr.Textbox(label=\"Question\", placeholder=\"Ask a question related to the PDF...\"),\n",
        "    ],\n",
        "    outputs= [gr.Textbox(label=\"Answer\")],\n",
        "\n",
        "    title=\"PDF Question-Answer Chatbot\",\n",
        "    description=\"Upload a PDF, then ask questions about the content.\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    iface.launch()\n"
      ],
      "metadata": {
        "id": "NPUq8699Ds0K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}